<section>
  <h1>Research Projects</h1>

  <article id="social">
    <div class="cell-image">
      <a href="images/projects/social/social-recursive.png" class="image thumb">
        <img src="images/projects/social/social-recursive.png"
             alt="Recursive estimation of other agent's goals." />
      </a>
    </div>
    <div class="cell-text">
      <h3>Modeling Social Interactions</h3>
      <p>Published in <i>CoRL-2021</i>, <i>NeurIPS-2021 Cooperative AI Workshop</i></p>
      <p>Robots must interact socially with humans. We present a computational framework that
        models social interactions as recursive Markov Decision Process (MDP). This formulation
        allows machines to understand what it means to help or hinder one another.
        Enabling robots to exhibit social skills could lead to smoother and more positive
        human-robot interactions. For instance, a more caring assistive robot.
        The model may also enable scientists to measure social interactions quantitatively,
        which could help scientists to study social impairements.
      </p>
      <p>[<a href="https://social-mdp.github.io/" target="_blank">Social MDP Website</a> | <a href="https://news.mit.edu/2021/robots-social-skills-1105" target="_blank">MIT News</a>]</p>
    </div>
  </article>

  <article id="ltl">
    <div class="cell-image">
      <a href="images/projects/ltl/ltl-grid.png" class="image thumb">
        <img src="images/projects/ltl/ltl-grid.png"
             alt="Robot follows a linear temporal logic command in the grid world." />
      </a>
      <a href="images/projects/ltl/ltl-fetch.png" class="image thumb hidden">
        <img src="images/projects/ltl/ltl-fetch.png"
             alt="Fetch robot follows a linear temporal logic command." />
      </a>
    </div>
    <div class="cell-text">
      <h3>Robots that Understand Linear Temporal Logic</h3>
      <p>Published in <i>IROS-2020</i>, <i>CoRL-2020</i>, <i>Frontiers Robotics & AI</i></p>
      <p>Robots must execute commands that are extended in time
        while being responsive to changes in their environments. A
        popular representation for such commands is linear temporal
        logic, LTL. We demonstrate a reinforcement learning (RL) agent
        which uses a compositional recurrent neural network that takes
        as input an LTL formula and determines satisfying actions.
        This RL agent also helps us learn programs that capture what sentences mean.</p>
    </div>
  </article>

  <article id="lang-planning">
    <div class="cell-image">
      <a href="images/projects/lang-planning/hsr-command.png" class="image thumb">
        <img src="images/projects/lang-planning/hsr-command.png"
             alt="HSR follows a natural language command." />
      </a>
    </div>
    <div class="cell-text">
      <h3>Robots that Understand Language</h3>
      <p>Published in <i>ICRA-2020</i>, <i>Findings of EMNLP 2021</i></p>
      <p>Executing natural language instructions from raw observations requires an integrated solution for language, perception, and planning. We demonstrate how an embodied agent uses reasoning about the future to follow natural language commands with an end-to-end compositional model trained on little data while generalizing to new environments.</p>
      <p>[<a href="https://techxplore.com/news/2020-03-robotic-planner-natural-language.html" target="_blank">Tech Xplore</a> | <a href="https://cbmm.mit.edu/news-events/news/combining-artificial-intelligence-their-passions-mit-news" target="_blank">MIT News</a>]</p>
    </div>
  </article>

  <article id="cs-integration">
    <div class="cell-image">
      <a href="images/projects/cs-integration/kbs.png" class="image thumb">
        <img src="images/projects/cs-integration/kbs.png"
             alt="Commonsense knowledge bases integrated in the system." />
      </a>
      <a href="images/projects/cs-integration/framework.png" class="image thumb hidden">
        <img src="images/projects/cs-integration/framework.png"
             alt="The proposed multiagent framework." />
      </a>
      <a href="https://youtu.be/6XG8HFLOEfs" data-poptrox="youtube,560x315" class="image thumb hidden">
        <img src="images/projects/analogyspace/projection.jpg" alt="Demo Video" />
      </a>
    </div>
    <div class="cell-text">
      <h3>Multiagent Reasoning System for Commonsense Knowledge Integration</h3>
      <p>2010/08 - 2012/01, Published in <i>ACM TiiS-2012</i></p>
      <p>While commonsense knowledge bases have been used in many intelligent systems, the quality of output results is limited by the coverage of single knowledge base. This multiagent reasoning system features a planning-based approach to integrating reasoning methods from multiple common sense knowledge bases to answer queries. The reasoning results of one reasoning method are passed to other reasoning methods to form a reasoning chain to the target context of a query. It also provides methods for developers to access the integrated reasoning results in real time.</p>
      <p>[<a href="https://www.youtube.com/watch?v=6XG8HFLOEfs" target="_blank">Application Video in Video Navigation</a> |
          <a href="https://docs.google.com/presentation/d/1voIWsJpDqDALwMu2FFYWds3g92KwT7aJvnKoa00vT4I/edit" target="_blank">Presentation in IUI-2013</a>]</p>
    </div>
  </article>

  <article id="language-explorer">
    <div class="cell-image">
      <a href="images/projects/language-explorer/context.png" class="image thumb">
        <img src="images/projects/language-explorer/context.png"
             alt="Context selection and vocabulary visualization." />
      </a>
      <a href="images/projects/language-explorer/vocabulary.png" class="image thumb hidden">
        <img src="images/projects/language-explorer/vocabulary.png"
             alt="Vocabulary page." />
      </a>
      <a href="images/projects/language-explorer/dialog.png" class="image thumb hidden">
        <img src="images/projects/language-explorer/dialog.png"
             alt="Dialogue learning." />
      </a>
      <a href="images/projects/language-explorer/sentence.png" class="image thumb hidden">
        <img src="images/projects/language-explorer/sentence.png"
             alt="Commonsense generated learning sentences." />
      </a>
    </div>
    <div class="cell-text">
      <h3>Language Explorer: Adaptive Language Learning Using Commonsense Knowledge</h3>
      <p>2011/10 - 2011/12, MIT Common Sense Reasoning for Interactive Applications Final Project</p>
      <p>Collaboration: Stephanie Leung (Harvard)</p>
      <p>Language Explorer is a mobile application that adapts to a learner’s context and capability. The Language Explorer leverages the crowdsourced commonsense knowledge base ConceptNet, the location based service Foursquare, and dialogues in online courses to automatically arrange materials and generate dialogues that fit the learner’s current situation.</p>
      <p>[<a href="pub/language-explorer.pdf" target="_blank">Final Paper</a>]</p>
    </div>
  </article>

  <article id="analogyspace">
    <div class="cell-image">
      <a href="images/projects/analogyspace/projection.jpg" class="image thumb">
        <img src="images/projects/analogyspace/projection.jpg"
             alt="The first two dimensions of Chinese AnalogySpace." />
      </a>
      <a href="https://youtu.be/5pR_4Us8cJU" data-poptrox="youtube,560x315" class="image thumb hidden">
        <img src="images/projects/analogyspace/projection.jpg" alt="Video of Chinese AnalogySpace Construction" />
      </a>
    </div>
    <div class="cell-text">
      <h3>Chinese ConceptNet & AnalogySpace</h3>
      <p>2009/09 - 2011/12, Published in <i>IJCAI-2011</i></p>
      <p>Collaboration: Rob Speer (MIT) on ConceptNet 5 Integration</p>
      <p>Using the data collected from social games, this research creates the largest Chinese commonsense knowledge base (over one million sentences) in the world. Collaborated with MIT Media Lab, this knowledge base is integrated into ConceptNet and demonstrates its reasoning capability with AnalogySpace. In addition to reasoning with one language, this research also features algorithms to make analogical reasoning across languages and generate new questions for crowdsourcing by comparing inference results in different knowledge bases.</p>
      <p>[<a href="https://www.youtube.com/watch?v=5pR_4Us8cJU" target="_blank">Demo Video</a> |
          Data available in <i><a href="https://conceptnet5.media.mit.edu/" target="_blank">ConceptNet 5</a></i>]</p>
    </div>
  </article>

  <article id="virtualpets">
    <div class="cell-image">
      <a href="images/projects/virtualpets/question.png" class="image thumb">
        <img src="images/projects/virtualpets/question.png"
             alt="Example question interface for collecting commonsense sentences." />
      </a>
      <a href="images/projects/virtualpets/homework.jpg" class="image thumb hidden">
        <img src="images/projects/virtualpets/homework.jpg"
             alt="Example homework interface for answer verification." />
      </a>
      <a href="https://youtu.be/9lqKrAPP-zw" data-poptrox="youtube,560x315" class="image thumb hidden">
        <img src="images/projects/virtualpets/question.png" alt="Demo Video" />
      </a>
    </div>
    <div class="cell-text">
      <h3>PTT Virtual Pets: Commonsense Knowledge Collection Game</h3>
      <p>2008/06 - 2009/08, Published in <i>HCOMP-2009</i></p>
      <p>Collaboration: Bani Chan, Kai-yang Chiang, Rax Wang</p>
      <p>Virtual Pets is a community-based question answering game built on PTT, the largest bulletin board system in Taiwan. The game creates mechanisms to utilize the rich interactions in the community to collect commonsense knowledge. The questions and answers in the game are exchanged between players to get more answers and verifications, which are shown as pets' homework. Using pets as virtual agents for community users, this game has successfully collected commonsense sentences and verifications since November 2008.</p>
      <p>[<a href="https://www.youtube.com/watch?v=9lqKrAPP-zw" target="_blank">Demo Video</a> |
          Game available on telnet://ptt.cc ->(P)lay ->(C)hicken]</p>
    </div>
  </article>
</section>

<section>
  <h1>Art & Technology Projects</h1>

  <article id="movisee">
  	<div class="cell-image">
  	  <a href="images/projects/movisee/all_collage.png" class="image thumb">
  	    <img src="images/projects/movisee/all_collage.png"
  	         alt="Examples of different users' output." />
  	  </a>
  	  <a href="images/projects/movisee/naama.png" class="image thumb hidden">
  	    <img src="images/projects/movisee/naama.png" alt="Abstract Film x Proformer" />
  	  </a>
  	  <a href="images/projects/movisee/divine.png" class="image thumb hidden">
  	    <img src="images/projects/movisee/divine.png" alt="Dancer plays MovISee" />
  	  </a>
  	  <a href="https://vimeo.com/113380410" data-poptrox="vimeo,500x375" class="image thumb hidden">
  	    <img src="images/projects/movisee/naama.png" alt="Abstract Film x Proformer" />
  	  </a>
  	  <a href="https://vimeo.com/121958778" data-poptrox="vimeo,500x375" class="image thumb hidden">
  	    <img src="images/projects/movisee/divine.png" alt="Dancer plays MovISee" />
  	  </a>
    </div>
    <div class="cell-text">
  	  <h3>MovISee</h3>
  	  <p>2014/09 - Present</p>
  	  <p>Collaboration: Yen-Ting Cho, Yen-Ting Yeh</p>
  	  <p>MovISee is a digital software and platform for people to play and create individual games and videos with their own existing digital files. We use depth camera to create mixed reality for people to explore the selected information and ultimately transform understanding of people’s body movements as means to composite customized visual outputs.</p>
  	  <p>[<a href="https://www.movisee.com/" target="_blank">Video Gallery</a>]</p>
    </div>
  </article>

  <article id="cubicfilm">
  	<div class="cell-image">
  	  <a href="images/projects/cubicfilm/categories.jpg" class="image thumb">
  	    <img src="images/projects/cubicfilm/categories.jpg"
  	         alt="Examples of tested subjects in each relative movement category." />
  	  </a>
  	  <a href="images/projects/cubicfilm/output1.jpg" class="image thumb hidden">
  	    <img src="images/projects/cubicfilm/output1.jpg" alt="Sample Output Image" />
  	  </a>
  	  <a href="images/projects/cubicfilm/redtokyo2_s.jpg" class="image thumb hidden">
  	    <img src="images/projects/cubicfilm/redtokyo2_s.jpg" alt="Sample Output Image" />
  	  </a>
  	  <a href="images/projects/cubicfilm/jimmy_s.jpg" class="image thumb hidden">
  	    <img src="images/projects/cubicfilm/jimmy_s.jpg" alt="Sample Output Image" />
  	  </a>
  	  <a href="https://vimeo.com/60115391" data-poptrox="vimeo,500x500" class="image thumb hidden">
  	    <img src="images/projects/cubicfilm/output1.jpg" alt="Sample Cubic Film" />
  	  </a>
  	  <a href="https://vimeo.com/58215334" data-poptrox="vimeo,500x250" class="image thumb hidden">
  	    <img src="images/projects/cubicfilm/output1.jpg" alt="Sample Cubic Film" />
  	  </a>
    </div>
    <div class="cell-text">
  	  <h3>Cubic Film</h3>
  	  <p>2013/01 - 2014/09</p>
  	  <p>Collaboration: Yen-Ting Cho, Yen-Ting Yeh</p>
  	  <p>Cubic film is a system to layer film frames into pixel cubes and provide different scanning mechanisms for users to experience moving images in new ways/dimensions. Each new scanning line produces a row of picture elements/pixels and disrupts the recorded time and spatial relationship.</p>
  	  <p>Sample cubic films:
  	  	<a href="https://vimeo.com/60115391" target="_blank">[1]</a> 
  	  	<a href="https://vimeo.com/58215334" target="_blank">[2]</a></p>
    </div>
  </article>

  <article id="handpaintfilm">
  	<div class="cell-image">
  	  <a href="images/projects/handpaintfilm/exp1.jpg" class="image thumb">
  	    <img src="images/projects/handpaintfilm/exp1.jpg" alt="Participant 1" />
  	  </a>
  	  <a href="images/projects/handpaintfilm/exp2.jpg" class="image thumb hidden">
  	    <img src="images/projects/handpaintfilm/exp2.jpg" alt="Participant 1" />
  	  </a>
  	  <a href="images/projects/handpaintfilm/output1.jpg" class="image thumb hidden">
  	    <img src="images/projects/handpaintfilm/output1.jpg" alt="Sample Output Film Strips" />
  	  </a>
  	  <a href="images/projects/handpaintfilm/output2.jpg" class="image thumb hidden">
  	    <img src="images/projects/handpaintfilm/output2.jpg" alt="Sample Output" />
  	  </a>
  	  <a href="https://youtu.be/LEQW4OEBNsY" data-poptrox="youtube,560x315" class="image thumb hidden">
  	    <img src="images/projects/handpaintfilm/exp1.jpg" alt="Sample Output Video" />
  	  </a>
    </div>
    <div class="cell-text">
  	  <h3>Hand Painted Film Plus</h3>
  	  <p>2012/01 - 2012/09</p>
  	  <p>Collaboration: Yen-Ting Cho, Yen-Ting Yeh</p>
  	  <p>Hand Painted Film Plus is a digital interface to create film strips, allowing participants to paint with camera, re-configuring the created sequence to explore new patterns and experiencing the transformation between time and space. Participants are especially surprised to see how static images moved.</p>
    </div>
  </article>
</section>

<section>
  <h1>Pilot / Exploratory Projects</h1>

  <article id="sciencevr">
    <div class="cell-image">
      <a href="https://youtu.be/r9gHfU5H_gc" data-poptrox="youtube,560x315" class="image thumb">
        <img src="images/projects/sciencevr/sciencevr.png" alt="Demo Video" />
      </a>
      <a href="images/projects/sciencevr/faradays-lab.png" class="image thumb hidden">
        <img src="images/projects/sciencevr/faradays-lab.png" alt="Faraday's Lab" />
      </a>
      <a href="images/projects/sciencevr/building-block.png" class="image thumb hidden">
        <img src="images/projects/sciencevr/building-block.png" alt="Building block of science experiments" />
      </a>
    </div>
    <div class="cell-text">
      <h3>ScienceVR: Science Experiments in VR</h3>
      <p>2016/10 - Present, <i><b>Finalist</b> of 2016 AT&T AR/VR Challenge</i></p>
      <p>Collaboration: Jackie Lee, Yero Yeh, Chih-Shiang
Chou</p>
      <p>ScienceVR is a VR experience for science education. We created an environment that allows users to do science experiments in VR. With real-time physics simulation, users can interact with virtual magnets and coils, watch them attracting and repelling each other, generate induced currents, and react to electrically-generated magnetic fields – all while seeing the magnetic fields through Michael Faraday's force lines. We believe learning science is like learning a new language. This immersive environment would be very helpful to learn sciences.</p>
      <p>[<a href="https://sciencevr.com/" target="_blank">Website</a> | <a href="https://www.youtube.com/watch?v=zSQ5m7J-lLI&feature=player_embedded" target="_blank">Faraday's Lab Demo Video</a> | <a href="doc/projects/faraday_lab_att_post.pdf" target="_blank">Introduction on AT&T Website</a>]</p>
    </div>
  </article>

  <article id="4dspacetime">
    <div class="cell-image">
      <a href="https://youtu.be/_0nOAnXavKk" data-poptrox="youtube,560x315" class="image thumb">
        <img src="images/projects/4dspacetime/memory-space.png" alt="Demo Video" />
      </a>
      <a href="images/projects/4dspacetime/interaction.png" class="image thumb hidden">
        <img src="images/projects/4dspacetime/interaction.png" alt="Interaction with 4DSpaceTime" />
      </a>
      <a href="images/projects/4dspacetime/web-service.png" class="image thumb hidden">
        <img src="images/projects/4dspacetime/web-service.png" alt="Web Service Interface" />
      </a>
    </div>
    <div class="cell-text">
      <h3>4DSpaceTime: Navigating Memories in VR</h3>
      <p>2015/10 - 2016/06</p>
      <p>Collaboration: Jackie Lee, Yero Yeh</p>
      <p>4DSpaceTime is a Virtual Reality experience inspired by Interstellar's tesseract scene. We re-construct this VR space from photos/videos - one's memories. Time becomes a physical dimension for you to travel forward/backward in time. You focus on one of your friends and families' faces and lean forward - like riding a Segway - to travel forward in time. The space itself was constructed based on relationships of photos and videos.</p>
      <p>[<a href="https://www.youtube.com/watch?v=_0nOAnXavKk&feature=player_embedded" target="_blank">Demo Video</a>]</p>
    </div>
  </article>

  <article id="taiwanuxd">
    <div class="cell-image">
      <a href="images/projects/taiwanuxd/home.png" class="image thumb">
        <img src="images/projects/taiwanuxd/home.png" alt="Home page" />
      </a>
      <a href="images/projects/taiwanuxd/overview.png" class="image thumb hidden">
        <img src="images/projects/taiwanuxd/overview.png" alt="Question and project overview" />
      </a>
      <a href="images/projects/taiwanuxd/questions.png" class="image thumb hidden">
        <img src="images/projects/taiwanuxd/questions.png" alt="Questions of a project" />
      </a>
      <a href="images/projects/taiwanuxd/secret-gender.png" class="image thumb hidden">
        <img src="images/projects/taiwanuxd/secret-gender.png" alt="Discovered secret by gender" />
      </a>
      <a href="images/projects/taiwanuxd/secret-profession.png" class="image thumb hidden">
        <img src="images/projects/taiwanuxd/secret-profession.png" alt="Discovered secret by profession" />
      </a>
      <a href="images/projects/taiwanuxd/user.png" class="image thumb hidden">
        <img src="images/projects/taiwanuxd/user.png" alt="User profile" />
      </a>
    </div>
    <div class="cell-text">
      <h3>TaiwanUXD.org</h3>
      <p>2014/10 - 2015/03</p>
      <p>Collaboration: Jackie Lee</p>
      <p>TaiwanUXD.org is a crowdsourcing platform for project owners to ask questions to community members. Through the user answers and profiles, the platform identifies secrets for project owners to learn user's behavior to refine their products. It is also a website to list the projects and members of Bay Area Taiwan User Experience and Design Group.</p>
    </div>
  </article>

  <article id="flora">
  	<div class="cell-image">
  	  <a href="https://youtu.be/JdAW3-qt-bo" data-poptrox="youtube,560x315" class="image thumb">
  	    <img src="images/projects/flora/logo.jpg" alt="Demo Video" />
  	  </a>
  	  <a href="images/projects/flora/telsoft-award.jpg" class="image thumb hidden">
  	    <img src="images/projects/flora/telsoft-award.jpg" alt="2010 Hinet Telsoft Competition 1st Place" />
  	  </a>
  	  <a href="images/projects/flora/appledaily.jpg" class="image thumb hidden">
  	    <img src="images/projects/flora/appledaily.jpg" alt="News on Apple Daily" />
  	  </a>
    </div>
    <div class="cell-text">
  	  <h3>Flora: Mobile Flower Image Recognition Service</h3>
  	  <p>2010/04 - 2011/07, <i><b>Gold Award</b> of 2010 Hinet Telsoft Competition and 8th YuShow Cup <b>Creativity Award</b></i></p>
  	  <p>Flora is an iPhone application for ordinary people to identify what a flower is. A user just takes a picture of the unknown flower, the app will automatically send the picture to the cloud server, process its images, and then send all information about this flower back to the user. Flora also has a built-in flower card collection game to promote interactions with flowers. Whenever a user finds a flower, the user is awarded with a flower card to share with friends.</p>
  	  <p>[<a href="https://www.youtube.com/watch?v=JdAW3-qt-bo&feature=player_embedded" target="_blank">Demo Video</a>]</p>
    </div>
  </article>
</section>

<section>
  <h1>Course Projects</h1>

  <article id="panorama">
  	<div class="cell-image">
  	  <a href="images/projects/panorama/architecture.jpg" class="image thumb">
  	    <img src="images/projects/panorama/architecture.jpg" alt="Example Output" />
  	  </a>
    </div>
    <div class="cell-text">
  	  <h3>Automatic Panoramic Image Stitching using CUDA</h3>
  	  <p>2010 Spring, <i>Parallel Programming Final Project</i></p>
  	  <p>This is an application for finding panorama automatically. We implemented Scale-Invariant Feature Transform (SIFT) feature extraction/match, gain compensation, and multi-band blending in Compute Unified Device Architecture (CUDA). Using CUDA, we improve the performance of Panoramic Image Stitching nearly 10 times faster. Our system can build the panorama in one minute with 6 1000x800 pixel input images, whereas the sequential version takes about 12 minutes to compute the same panorama.</p>
  	  <p>[<a href="https://www.csie.ntu.edu.tw/~r98922037/pp/pp_report_group2.pdf" target="_blank">Report</a> |
  	  	  <a href="https://www.csie.ntu.edu.tw/~r98922037/pp/pp_slide_group2.pdf" target="_blank">Slide</a>]</p>
    </div>
  </article>

  <article id="sdio">
  	<div class="cell-image">
  	  <a href="images/projects/dsd/sdio-card.jpg" class="image thumb">
  	    <img src="images/projects/dsd/sdio-card.jpg" alt="SDIO Card With RTC" />
  	  </a>
  	  <a href="images/projects/dsd/block-diagram.png" class="image thumb hidden">
  	    <img src="images/projects/dsd/block-diagram.png" alt="Block diagram" />
  	  </a>
    </div>
    <div class="cell-text">
  	  <h3>SDIO Card with World Real-Time Clock/Alarm</h3>
  	  <p>2007 Fall, <i>Digital System Design Final Project</i></p>
  	  <p>This project simulates a Secure Digital Input Output (SDIO) card interface to control and display a world real-time clock on LCD. It supports three functionalities: real-time clock, alarm, and change time zone. The card can operate with battery only; there is no need to connect it to a SDIO host.</p>
  	  <p>[<a href="https://www.csie.ntu.edu.tw/~b94029/Courses/DSD/DSD_Group4/" target="_blank">Website</a>]</p>
    </div>
  </article>
</section>
